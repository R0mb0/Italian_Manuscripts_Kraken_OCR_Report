\documentclass[11pt,a4paper]{article}

% Pacchetti per la lingua e la codifica
\usepackage[utf8]{inputenc}
\usepackage[italian]{babel}
\usepackage[T1]{fontenc}

% Pacchetti matematici e grafici
\usepackage{amsmath,amssymb}
\usepackage[pdftex]{graphicx}
\usepackage[svgnames]{xcolor}
\usepackage{float} % Per posizionare meglio le immagini/tabelle

% Layout e formattazione
\usepackage[margin=2.5cm]{geometry}
\usepackage{parskip}
\usepackage{enumitem}
\usepackage{setspace}
\onehalfspacing % Interlinea per migliore leggibilità (stile relazione)

% Pacchetti per codice e link
\usepackage{hyperref}
\hypersetup{
	colorlinks=true,
	linkcolor=DarkBlue,
	filecolor=magenta,      
	urlcolor=cyan,
	pdftitle={Report Progetto IA},
}
\usepackage{minted}
\setminted{
	frame=single,
	linenos,
	fontsize=\small,
	breaklines,
	breakanywhere,
	numbersep=4pt,
	xleftmargin=6pt,
	framesep=2pt
}

% Bibliografia
\usepackage{csquotes}
\usepackage{biblatex}
\addbibresource{riferimenti.bib}

% Intestazione
\title{\textbf{Applicazione di Reti Neurali Ricorrenti e Convoluzionali per la Digitalizzazione di Archivi Clinici Storici}}
\author{Francesco Rombaldoni\\
	\small Corso di Applicazioni dell'Intelligenza Artificiale: Biologia e Medicina\\
	\small Universit\`a degli Studi di Urbino Carlo Bo\\
	\small \texttt{f.rombaldoni@campus.uniurb.it}}
\date{\today}

\begin{document}
	
	\maketitle
	
	\begin{abstract}
		\noindent Il presente elaborato descrive la progettazione e l'implementazione di un sistema di \emph{Handwritten Text Recognition} (HTR) basato su Deep Learning, sviluppato per le esigenze archivistiche dell'INRCA (Istituto Nazionale di Riposo e Cura per Anziani). Il progetto affronta la sfida della dematerializzazione di documenti sanitari storici, vincolata da stringenti normative sulla privacy (GDPR) che impediscono l'uso di servizi cloud commerciali. Viene presentata una pipeline completa che utilizza il framework open-source \textbf{Kraken}: dalla pre-elaborazione delle immagini (binarizzazione, deskewing) all'addestramento di un modello ibrido CNN-LSTM. Particolare attenzione è dedicata all'analisi dell'architettura neurale, che combina reti convoluzionali per l'estrazione di feature visive e reti ricorrenti per la modellazione sequenziale del testo, evidenziando come queste tecnologie risolvano le criticità legate alla variabilità della grafia manuscritta in ambito medico.
	\end{abstract}
	
	\tableofcontents
	\newpage
	
	\section{Introduzione e Contesto Operativo}
	
	L'applicazione delle tecnologie di Intelligenza Artificiale in ambito sanitario non riguarda esclusivamente la diagnostica o la genomica, ma gioca un ruolo cruciale anche nella gestione del patrimonio informativo (Information Retrieval e Data Mining).
	Questo progetto nasce all'interno dell'INRCA, un Istituto di Ricovero e Cura a Carattere Scientifico (IRCCS) pubblico, che si trova a dover gestire vasti archivi cartacei storici contenenti dati sensibili dei pazienti.
	
	\subsection{Il Problema: Normativa e Conservazione}
	La gestione di tali archivi è soggetta a due forze contrapposte:
	\begin{itemize}
		\item \textbf{Valore Storico-Scientifico:} Le cartelle cliniche decennali contengono informazioni preziose per studi retrospettivi longitudinali (es. evoluzione di patologie geriatriche).
		\item \textbf{Vincoli Normativi (GDPR):} Il Regolamento UE 2016/679 impone rigorosi limiti alla conservazione dei dati personali ("diritto all'oblio") e standard di sicurezza elevati. Spesso, la normativa impone la distruzione del supporto fisico dopo un certo periodo (es. 10, 20 o 40 anni), rischiando di perdere per sempre l'informazione se non adeguatamente digitalizzata.
	\end{itemize}
	L'obiettivo è quindi creare una "copia digitale fedele" e ricercabile che permetta di distruggere il cartaceo mantenendo l'informazione, in conformità agli standard ISO/IEC 21964.
	
	\subsection{Analisi delle Soluzioni Esistenti e Scelta Tecnologica}
	Inizialmente sono state valutate soluzioni commerciali (OCR tradizionali come ABBYY o servizi Cloud come Google Vision API). Queste sono state scartate per due motivi principali:
	\begin{enumerate}
		\item \textbf{Privacy e Sovranità dei Dati:} L'invio di scansioni contenenti dati sanitari verso server cloud di terze parti (spesso extra-UE) presenta criticità insormontabili per un ente pubblico. È necessario un sistema \emph{on-premise} (locale).
		\item \textbf{Limiti degli OCR tradizionali:} I motori OCR classici funzionano bene su testo stampato, ma falliscono sul manoscritto corsivo (\emph{Handwritten Text Recognition} - HTR), dove la segmentazione dei singoli caratteri è ambigua.
	\end{enumerate}
	
	La scelta è ricaduta su \textbf{Kraken}, un motore HTR open-source basato su reti neurali, che permette l'addestramento fine-grained su specifiche calligrafie e l'esecuzione interamente locale.
	
	\section{Fondamenti Teorici dell'Architettura}
	
	Il cuore del sistema Kraken si basa su un'architettura ibrida che rispecchia i modelli di Deep Learning analizzati durante il corso: l'integrazione di Reti Convoluzionali (CNN) e Reti Ricorrenti (RNN).
	
	\subsection{Dall'Immagine alla Sequenza: CNN e Feature Extraction}
	Come visto nelle lezioni sull'analisi di bio-immagini, le \textbf{CNN (Convolutional Neural Networks)} sono eccellenti nell'estrarre caratteristiche spaziali locali.
	Nel contesto dell'HTR, l'input non è l'intera pagina, ma una singola riga di testo normalizzata in altezza. La CNN scorre lungo questa "striscia" di pixel applicando filtri (kernel) che rilevano pattern primitivi: tratti verticali, curve, occhielli.
	L'output della CNN non è una classificazione ("questo è un gatto"), ma una sequenza di vettori di feature (\emph{feature maps}) che rappresentano la morfologia del testo lungo l'asse orizzontale.
	
	\subsection{Modellazione del Contesto: RNN e LSTM}
	La scrittura è intrinsecamente sequenziale: la forma di una lettera è influenzata da quelle adiacenti (legature del corsivo) e l'interpretazione di un segno ambiguo dipende dal contesto della parola.
	Per questo, le feature estratte dalla CNN vengono passate a una **RNN (Recurrent Neural Network)**. Nello specifico, si utilizzano celle **LSTM (Long Short-Term Memory)** Bidirezionali (BLSTM).
	\begin{itemize}
		\item \textbf{Memoria a Lungo Termine:} Le LSTM risolvono il problema della "scomparsa del gradiente" (\emph{vanishing gradient}) tipico delle RNN semplici, permettendo alla rete di ricordare il contesto anche a distanza di molti caratteri.
		\item \textbf{Bidirezionalità:} La rete legge la riga sia da sinistra a destra che viceversa, utilizzando il contesto futuro e passato per disambiguare il carattere corrente.
	\end{itemize}
	
	\subsection{Allineamento e Loss Function (CTC)}
	Un problema classico nel riconoscimento del manoscritto è che non sappiamo "dove" inizia e finisce esattamente un carattere nell'immagine. Per evitare la costosa operazione di segmentare manualmente ogni lettera, si utilizza la **Connectionist Temporal Classification (CTC)**.
	La CTC permette di addestrare la rete fornendo solo l'immagine della riga e la stringa di testo corrispondente. La funzione di loss calcola la probabilità di tutti i possibili allineamenti tra la sequenza di output della rete e il testo target, massimizzando la probabilità della trascrizione corretta.
	
	\section{Metodologia Sperimentale}
	
	Il progetto ha seguito una pipeline rigorosa di preparazione dei dati e addestramento, fondamentale per ottenere risultati validi in sistemi di Machine Learning supervisionati.
	
	\subsection{Acquisizione e Pre-processing (Data Cleaning)}
	La qualità del dataset è determinante. Sono state selezionate circa 150 pagine di documenti rappresentativi.
	La fase di pre-processing ha utilizzato il tool \emph{ScanTailor} per operazioni di pulizia essenziali:
	\begin{enumerate}
		\item \textbf{Deskewing:} Correzione dell'inclinazione della pagina per garantire righe orizzontali.
		\item \textbf{Binarizzazione:} Conversione dell'immagine in bianco e nero puro. Questo passaggio rimuove il rumore di fondo (ingiallimento della carta, macchie) e isola l'inchiostro, facilitando il lavoro della CNN.
		\item \textbf{Segmentazione Layout:} Definizione dell'area utile contenente il testo, scartando bordi e note marginali irrilevanti.
	\end{enumerate}
	
	\subsection{Annotazione e Creazione del Ground Truth}
	Per l'addestramento supervisionato è necessario un \emph{Ground Truth} (GT) affidabile. Le immagini processate sono state caricate in un ambiente di annotazione dove è stata effettuata:
	\begin{itemize}
		\item \textbf{Segmentazione delle righe:} Un algoritmo basato su proiezioni orizzontali ha individuato le righe di testo (baselines).
		\item \textbf{Trascrizione Manuale:} Ogni riga è stata trascritta manualmente. Questa fase è stata critica per definire le convenzioni di trascrizione (es. come trattare le correzioni, le abbreviazioni mediche, la punteggiatura).
	\end{itemize}
	Il risultato è un dataset di coppie (immagine riga, testo UTF-8) salvato in formato XML/ALTO standard.
	
	\subsection{Strategia di Training: Fine-Tuning}
	Considerata la dimensione ridotta del dataset (150 pagine), l'addestramento di una rete profonda da zero (\emph{from scratch}) avrebbe portato quasi certamente all'\textbf{overfitting} (la rete impara a memoria i dati di training ma non generalizza su nuovi dati).
	Si è quindi adottata una tecnica di \textbf{Transfer Learning}:
	\begin{itemize}
		\item Si è partiti da un modello Kraken pre-addestrato su un ampio corpus generico.
		\item Si è effettuato il \emph{fine-tuning} (raffinamento dei pesi) utilizzando il dataset specifico dell'INRCA.
	\end{itemize}
	Questo approccio ha permesso alla rete di sfruttare la conoscenza pregressa su come riconoscere forme base di caratteri, adattandosi poi allo stile calligrafico specifico e al lessico medico dei documenti target.
	
	\section{Risultati e Discussione}
	
	Il modello è stato valutato su un set di validazione separato, non utilizzato durante la fase di training, per stimarne le reali capacità in produzione.
	
	\subsection{Metriche di Valutazione}
	L'accuratezza è stata misurata utilizzando metriche standard per l'OCR:
	\begin{itemize}
		\item \textbf{Character Error Rate (CER):} Percentuale di caratteri errati (inseriti, cancellati o sostituiti).
		\item \textbf{Word Error Rate (WER):} Percentuale di parole errate.
	\end{itemize}
	Il sistema ha raggiunto un'accuratezza media di circa l'\textbf{88\%} sul riconoscimento dei caratteri. Un risultato promettente considerando la complessità della grafia e la bassa qualità dei supporti originali.
	
	\subsection{Analisi degli Errori}
	Dall'analisi qualitativa degli errori emergono pattern interessanti:
	\begin{itemize}
		\item \textbf{Ambiguità morfologiche:} Lettere simili (es. 'u' e 'n', 'a' e 'o') vengono confuse se scritte frettolosamente. Qui la componente LSTM (contesto) aiuta, ma non sempre riesce a disambiguare se la parola è un termine medico non comune o un nome proprio.
		\item \textbf{Rumore residuo:} Macchie di inchiostro o timbri sovrapposti al testo causano "allucinazioni" della rete, che cerca di interpretare il rumore come lettere.
	\end{itemize}
	
	\section{Conclusioni e Sviluppi Futuri}
	
	Il progetto ha dimostrato che è possibile applicare tecniche avanzate di Deep Learning (CNN+LSTM) per risolvere problemi concreti di gestione documentale in ambito sanitario, rispettando i vincoli di privacy imposti dal GDPR tramite elaborazione locale.
	
	Per elevare il sistema a un livello produttivo industriale, sono state identificate le seguenti linee di sviluppo:
	\begin{enumerate}
		\item \textbf{Data Augmentation:} Aumentare sinteticamente il dataset di training (applicando rotazioni, rumore e deformazioni elastiche alle immagini esistenti) per migliorare la robustezza del modello senza dover annotare manualmente migliaia di nuove pagine.
		\item \textbf{Post-processing con Language Models:} Integrare l'output dell'OCR con un modello linguistico probabilistico addestrato su corpora medici. Questo permetterebbe di correggere errori come "paziende" in "paziente" basandosi sulla probabilità statistica delle sequenze di parole, similmente a come operano i moderni correttori ortografici intelligenti.
		\item \textbf{Integrazione Workflow:} Sviluppare un'interfaccia utente che permetta agli archivisti di validare rapidamente le trascrizioni incerte, creando un ciclo virtuoso (\emph{human-in-the-loop}) dove le correzioni umane vengono usate per ri-addestrare e migliorare continuamente il modello.
	\end{enumerate}
	
	\printbibliography
	
\end{document}